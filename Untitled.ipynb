{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM25YyZCduSqo38JdONwg7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishgeorge1811/nlpchatbot/blob/master/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_uJY8arOmo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing nltk library\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aabxfVjvPctK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePdWbpQKiC29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2.\tStep 2\n",
        "#Importing the text where NLP techniques are applying and converting the capital letters to small letters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCUiIMoaPcwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e49633b3-6668-45c1-d17a-cfe0bdcc491e"
      },
      "source": [
        "text = \"NLP or Natural Language Processing  is a field that is concerned to describe the ability of a computer to understand, analyze, manipulate, and potentially generate human language. Financial Industries are providing good service to customers for financial consultation by creating chatbots that reply answers promptly to the questions of customers in a human manner. It is estimated that the Healthcare Industry contains 80 % of the data related to the patients are in an unstructured format.To conduct research and development on drugs efficiently, improve treatment standards, and validating the outcomes of treatments given to the patients simply and as automatically as possible NLP plays a vital role. Natural language processing is a magical looking glass through which the digital marketers can be able to observe what the customers wish to purchase. And brand perception is done through social listening by analyzing the large scale of text data related to the customers’ details. Tokenization is a process of splitting a paragraph, sentence, phrase into small units, and each unit is called as tokens. Depending on the selection, the tokens can be a sentence, word, etc. Punctuations are marks like a full stop, comma, colon, special characters, etc to make clear the sentence meaningfully in writing sentences. Stemming is a process of reducing the inflective words to their root word or word stem. By using stemming algorithms it explicitly correlates words with similar meaning.  Lemmatization Algorithm is the same as the Stemming Algorithm, the only difference is that it will reduce the reflective words to meaningful words.A count vectorizer is a collection of text documents to a vector. It enables the pre-processing of text data to generate the vector representation. This functionality makes it a highly flexible feature representation module for text.  TF-IDF as, a measure of the originality of a word by comparing the number of times a word appears in a document with the number of documents the words appear in.  \"\n",
        "text.lower()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'nlp or natural language processing  is a field that is concerned to describe the ability of a computer to understand, analyze, manipulate, and potentially generate human language. financial industries are providing good service to customers for financial consultation by creating chatbots that reply answers promptly to the questions of customers in a human manner. it is estimated that the healthcare industry contains 80 % of the data related to the patients are in an unstructured format.to conduct research and development on drugs efficiently, improve treatment standards, and validating the outcomes of treatments given to the patients simply and as automatically as possible nlp plays a vital role. natural language processing is a magical looking glass through which the digital marketers can be able to observe what the customers wish to purchase. and brand perception is done through social listening by analyzing the large scale of text data related to the customers’ details. tokenization is a process of splitting a paragraph, sentence, phrase into small units, and each unit is called as tokens. depending on the selection, the tokens can be a sentence, word, etc. punctuations are marks like a full stop, comma, colon, special characters, etc to make clear the sentence meaningfully in writing sentences. stemming is a process of reducing the inflective words to their root word or word stem. by using stemming algorithms it explicitly correlates words with similar meaning.  lemmatization algorithm is the same as the stemming algorithm, the only difference is that it will reduce the reflective words to meaningful words.a count vectorizer is a collection of text documents to a vector. it enables the pre-processing of text data to generate the vector representation. this functionality makes it a highly flexible feature representation module for text.  tf-idf as, a measure of the originality of a word by comparing the number of times a word appears in a document with the number of documents the words appear in.  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOXilGpOiIrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3.\tStep 3\n",
        "#Tokenize the text into sentence and tokenize the text into words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX8Qx7MoPc1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "words = nltk.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0HkPFYwftDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step4\n",
        "##Initializing the WordNetLemmatizer  and stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjr7BWILh7QN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wn = nltk.stem.WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8fWG4fcftIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step5 \n",
        "#Define a function called LemTokens which will take as input the tokens and return normalized tokens.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7eGvfXOiyXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LemTokens(tokens):\n",
        "   return [wn.lemmatize(words) for words in tokens if words not in set(stopwords.words('english')) ]\n",
        "   remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm486VUZiyaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step6\n",
        "##Method to clean up and tokenize the text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt3ePctxiyhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LemNormalize(text):\n",
        "   return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajmYikeViyfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step7\n",
        "#Importing the cosine similarity and countvectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziAFWAYliyd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgQxzErBftLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 8\n",
        "#Define a function for, a greeting by the bot.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2RKCCQJj3n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GREETING_INPUTS = ('hi','hello')\n",
        "GREETING_RESPONSES = ['hi','hello','I am glad! You are talking to me','hey']\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T9ub6Ssj3qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step 9\n",
        "#To get a response from our bot for input questions,we made a function response which finds the user’s utterance for one or more known keywords and returns one of the possible response. If it doesn’t find the input matching in any of the keywords, it returns a response:” I am sorry! I don’t understand you”."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8HsdRamj3v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#user input and check for the similarity with the sentences in the text.\n",
        "def response(user_response):\n",
        "        robo_response = \"\"\n",
        "#adding the sentence corpus to user response\n",
        "        sentences.append(user_response)        \n",
        "#Initializing the CountVectorizer \n",
        "        cv = CountVectorizer ()\n",
        "#Tranforming the vector to numeric \n",
        "        X = cv.fit_transform(sentences)  \n",
        "#Calculating the cosine similarity of the  user input with the entire CountVerter X. \n",
        "        vals_cv = cosine_similarity(X[-1],X)\n",
        "#sorting the indexes based on increasing similarity  \n",
        "        indx_of_most_similar_sentence = vals_cv.argsort()[0][-2]\n",
        "#Flattening the cosine similarity array into a vector of rows.      \n",
        "        flat_vals_cv = vals_cv.flatten()   \n",
        "#Sorting the values in increasing order of cosine similarities.       \n",
        "        flat_vals_cv.sort()   \n",
        "# Storing the second highest cosine similarity value.\n",
        "        highest_similarity = flat_vals_cv[-2] \n",
        "#If the second highest cosine similarity value is zero it means there is no match\n",
        "        if(highest_similarity == 0):    \n",
        "              robo_response = \"I am sorry! I don't understand you\"\n",
        "              return robo_response\n",
        "        else:\n",
        "# displays the matched line from the text to the user.\n",
        "              robo_response = sentences[indx_of_most_similar_sentence]  \n",
        "              return robo_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIQG7ks2mVOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step 10\n",
        "#Finally, we will feed the comments that we want our bot to say while starting and ending the conversation depending upon user’s input."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMy30c9VftGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about NLP. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sentences.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BEZssCgu0pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}